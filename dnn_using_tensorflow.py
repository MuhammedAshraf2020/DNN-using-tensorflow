# -*- coding: utf-8 -*-
"""DNN using tensorflow ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lSVEpQQtMQJec0GVxBsDzXFlfi2gPXVX
"""

#import libs
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.datasets.mnist import load_data

# prepare dataset

(X_train , y_train) , (X_test , y_test) = load_data()
X_train = X_train.astype("float32") / 255
X_test = X_test.astype("float32") / 255
# Make sure images have shape (28, 28, 1)
X_train = np.expand_dims(X_train, -1)
X_test = np.expand_dims(X_test, -1)

for i in range(0, 9):
    plt.subplot(330 + 1 + i)
    plt.imshow(X_train[i][: , : , 0], cmap=plt.get_cmap('gray'))

plt.show()

X_train = [X_train[i].ravel() for i in range(len(X_train))]

X_test = [X_test[i].ravel() for i in range(len(X_test))]

y_train = tf.keras.utils.to_categorical(y_train , num_classes = 10)
y_test  = tf.keras.utils.to_categorical(y_test , num_classes = 10 )

#set parameter
n_input = 28 * 28
n_hidden_1 = 512
n_hidden_2 = 256
n_hidden_3 = 128
n_output   = 10

learning_rate = 0.01 
epochs = 50
batch_size = 128

tf.compat.v1.disable_eager_execution()

# weight intialization

X = tf.compat.v1.placeholder(tf.float32 , [None , n_input])
y = tf.compat.v1.placeholder(tf.float32 , [None , n_output])

def Weights_init(list_layers , stddiv):
  Num_layers = len(list_layers)
  weights = {}
  bias    = {}
  for i in range( Num_layers-1):
    weights["W{}".format(i+1)] = tf.Variable(tf.compat.v1.truncated_normal([list_layers[i] , list_layers[i+1]] ,  stddev = stddiv))
    bias["b{}".format(i+1)] = tf.Variable(tf.compat.v1.truncated_normal([list_layers[i+1]]))
  return weights , bias

list_param = [784 , 512 , 256 , 128 , 10]

weights , biases = Weights_init(list_param , 0.1)

def Model (X , nn_weights , nn_bias):
  Z1 = tf.add(tf.matmul(X , nn_weights["W1"]) , nn_bias["b1"])
  Z1_out = tf.nn.relu(Z1)

  Z2 = tf.add(tf.matmul(Z1_out , nn_weights["W2"]) , nn_bias["b2"])
  Z2_out = tf.nn.relu(Z2)

  Z3 = tf.add(tf.matmul(Z2_out , nn_weights["W3"]) , nn_bias["b3"])
  Z3_out = tf.nn.relu(Z3)

  Z4 = tf.add(tf.matmul(Z3_out , nn_weights["W4"]) , nn_bias["b4"])
  Z4_out = tf.nn.softmax(Z4)
  return Z4_out

nn_layer_output = Model(X , weights , biases)

loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits = nn_layer_output , labels = y))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)

init = tf.compat.v1.global_variables_initializer()

# Determining if the predictions are accurate
is_correct_prediction = tf.equal(tf.argmax(nn_layer_output , 1),tf.argmax(y, 1))

#Calculating prediction accuracy
accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))

saver = tf.compat.v1.train.Saver()

with tf.compat.v1.Session() as sess:
      # initializing all the variables
    sess.run(init)
    total_batch = int(len(X_train) / batch_size)
    for epoch in range(epochs):
        avg_cost = 0
        for i in range(total_batch):
            batch_x  , batch_y = X_train[i * batch_size : (i + 1) * batch_size] , y_train[i * batch_size : (i + 1) * batch_size]
            _, c = sess.run([optimizer,loss], feed_dict={X: batch_x, y: batch_y})
            avg_cost += c / total_batch
        if(epoch % 10 == 0):
          print("Epoch:", (epoch + 1), "train_cost =", "{:.3f} ".format(avg_cost) , end = "")
          print("train_acc = {:.3f} ".format(sess.run(accuracy, feed_dict={X: X_train, y:y_train})) , end = "")
          print("valid_acc = {:.3f}".format(sess.run(accuracy, feed_dict={X: X_test, y:y_test})))
    saver.save(sess , save_path = "/content/Model.ckpt")